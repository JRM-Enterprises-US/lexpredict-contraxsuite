#shared-configmaps

apiVersion: v1
data:
  uwsgi.ini: |-
    [uwsgi]
    master = true
    vacuum = true
    socket = 0.0.0.0:3031
    plugins-dir = /usr/lib/uwsgi/plugins
    plugins = python3
    protocol = uwsgi
    buffer-size = 65535
    wsgi = wsgi:application
    static-map = /explorer/static=/contraxsuite_services/staticfiles
    static-expires = /* 7776000
    offload-threads = %k
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: uwsgi-ini-configmap

---

apiVersion: v1
data:
  local_settings.py: |-
    SECRET_KEY = 'Welcome1'
    AUTOLOGIN = False
    HOST_NAME = '{{ .Values.domain_name }}'
    FRONTEND_ROOT_URL = '{{ .Values.domain_name }}'
    BASE_URL = '/explorer/'
    DEBUG_SQL = False
    DEBUG_TEMPLATE = False
    EMAIL_HOST = 'localhost'
    EMAIL_BACKEND = 'django.core.mail.backends.console.EmailBackend'
    EMAIL_USE_TLS = False
    EMAIL_PORT = 587
    EMAIL_HOST_USER = ''
    EMAIL_HOST_PASSWORD = ''
    ADMINS = (
        ('Administrator', 'admin@localhost')
    )
    DATABASES = {
        'default': {
            'ENGINE': 'django.db.backends.postgresql_psycopg2',
            'NAME': '{{ .Values.pg_db_name }}',
            'USER': '{{ .Values.pg_user }}',
            'PASSWORD': '{{ .Values.pg_password }}',
            'HOST': '{{ .Values.pg_host_name }}',
            'PORT': '5432',
            'CONN_MAX_AGE': 500
        },
    }
    ALLOWED_HOSTS = (
        '*'
    )
    INTERNAL_IPS = (
        '127.0.0.1',
        'localhost'
    )
    DEBUG = False
    PIPELINE_ENABLED = False
    CELERY_BROKER_URL = 'amqp://contrax1:contrax1@contrax-rabbitmq:5672/contrax1_vhost'
    CELERY_CACHE_REDIS_URL = 'redis://contrax-redis:6379/0'
    ELASTICSEARCH_CONFIG = {
        'hosts': [{'host': 'contrax-elasticsearch', 'port': 9200}],
        'index': 'contraxsuite'
    }
    STATICFILES_DIRS = (
        '/static',
    )
    MEDIA_ROOT = '/data/media'
    DATA_ROOT = '/data/data'
    import platform
    import sys
    from contraxsuite_logging import ContraxsuiteJSONFormatter
    DEBUG_STACK_DUMP_DIR = '/data/logs'
    LOGGING = {
        'version': 1,
        'disable_existing_loggers': False,
        'formatters': {
            'verbose': {
                '()': 'contraxsuite_logging.ContraxsuiteTextFormatter',
                'format': '%(levelname)-7s %(asctime)s | %(message)s'
            },
            'json': {
                '()': 'contraxsuite_logging.ContraxsuiteJSONFormatter'
            }
        },
        'handlers': {
            'console': {
                'level': 'DEBUG',
                'filters': [],
                'class': 'logging.StreamHandler',
                'formatter': 'json',
                'stream': sys.stdout,
            },
        },
        'loggers': {
            'apps.task.models': {
                'handlers': ['console'],
                'level': 'INFO',
                'propagate': False,
            },
            'apps.task.tasks': {
                'handlers': ['console'],
                'level': 'INFO',
                'propagate': False,
            },
            'django': {
                'handlers': ['console'],
                'level': 'INFO',
                'propagate': False,
            },
            'celery.app.trace': {
                'handlers': ['console'],
                'level': 'INFO',
                'propagate': False,
            },
            'django.db.backends': {
                'handlers': (['console'] if DEBUG_SQL else []),  # Quiet by default!
                'propagate': False,
                'level': 'DEBUG' if DEBUG_SQL else 'ERROR',
            },
            'frontend': {
                'handlers': ['console'],
                'level': 'INFO',
                'propagate': False,
            }
        },
    }
    CONTRAX_FILE_STORAGE_TYPE = 'WebDAV'
    CONTRAX_FILE_STORAGE_WEBDAV_ROOT_URL = 'http://contrax-webdav:80'
    CONTRAX_FILE_STORAGE_WEBDAV_USERNAME = 'user'
    CONTRAX_FILE_STORAGE_WEBDAV_PASSWORD = 'password'
    ACCOUNT_EMAIL_VERIFICATION='optional'
    TIKA_SERVER_ENDPOINT = ''
    TIKA_JAR_BASE_PATH = '/contraxsuite_services/jars'
    TIKA_PARSE_MODE = 'plain'
    MLFLOW_PIP_ENV = '/contraxsuite_services/venv'
    MLFLOW_S3_ENDPOINT_URL = 'http://contrax-minio:9000' or None
    AWS_ACCESS_KEY_ID = 'Administrator'
    AWS_SECRET_ACCESS_KEY = 'Administrator'
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: local-settings-py-configmap

---
{{ if eq .Values.pg_host_name "contrax-db" }}
#contrax-db

apiVersion: v1
data:
  backup-cron.conf: "# m h d month weekday user command\n\n0 0\t* * * root bash /contraxsuite/db-backup.sh\n"
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: pg-backup-cron-722aba991ba1ea855e6736133174990a

---

apiVersion: v1
data:
  postgresql.conf: |-
    listen_addresses = '*'
    max_connections = 1000
    shared_buffers = 1536MB
    work_mem = 7864kB
    maintenance_work_mem = 384MB
    dynamic_shared_memory_type = posix
    effective_io_concurrency = 200
    max_worker_processes = 4
    max_parallel_workers_per_gather = 2
    max_wal_size = 4GB
    min_wal_size = 2GB
    checkpoint_completion_target = 0.9
    random_page_cost = 1.1
    effective_cache_size = 4608MB
    default_statistics_target = 100
    logging_collector = on
    log_rotation_age = 1d
    log_min_duration_statement = 5000
    log_line_prefix = '%t [%p] '
    log_timezone = 'UTC'
    datestyle = 'iso, mdy'
    timezone = 'UTC'
    lc_messages = 'en_US.utf8'
    lc_monetary = 'en_US.utf8'
    lc_numeric = 'en_US.utf8'
    lc_time = 'en_US.utf8'
    default_text_search_config = 'pg_catalog.english'
    max_connections = 1000
    shared_buffers = 2GB
    effective_cache_size = 6GB
    maintenance_work_mem = 512MB
    checkpoint_completion_target = 0.9
    wal_buffers = 16MB
    default_statistics_target = 100
    random_page_cost = 4
    effective_io_concurrency = 2
    work_mem = 524kB
    min_wal_size = 1GB
    max_wal_size = 2GB
    max_worker_processes = 4
    max_parallel_workers_per_gather = 2
    shared_preload_libraries = 'pg_stat_statements'
    pg_stat_statements.max = 1000
    pg_stat_statements.track = all
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: postgresqlconf-configmap

---

apiVersion: v1
data:
  postgresql.conf: |-
    listen_addresses = '*'
    max_connections = 1000
    shared_buffers = 1536MB
    work_mem = 7864kB
    maintenance_work_mem = 384MB
    dynamic_shared_memory_type = posix
    effective_io_concurrency = 200
    max_worker_processes = 4
    max_parallel_workers_per_gather = 2
    max_wal_size = 4GB
    min_wal_size = 2GB
    checkpoint_completion_target = 0.9
    random_page_cost = 1.1
    effective_cache_size = 4608MB
    default_statistics_target = 100
    logging_collector = on
    log_rotation_age = 1d
    log_min_duration_statement = 5000
    log_line_prefix = '%t [%p] '
    log_timezone = 'UTC'
    datestyle = 'iso, mdy'
    timezone = 'UTC'
    lc_messages = 'en_US.utf8'
    lc_monetary = 'en_US.utf8'
    lc_numeric = 'en_US.utf8'
    lc_time = 'en_US.utf8'
    default_text_search_config = 'pg_catalog.english'
    max_connections = 1000
    shared_buffers = 2GB
    effective_cache_size = 6GB
    maintenance_work_mem = 512MB
    checkpoint_completion_target = 0.9
    wal_buffers = 16MB
    default_statistics_target = 100
    random_page_cost = 4
    effective_io_concurrency = 2
    work_mem = 524kB
    min_wal_size = 1GB
    max_wal_size = 2GB
    max_worker_processes = 4
    max_parallel_workers_per_gather = 2
    shared_preload_libraries = 'pg_stat_statements,powa,pg_stat_kcache,pg_qualstats'
    pg_stat_statements.max = 1000
    pg_stat_statements.track = all
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: postgresqlconf-powa-configmap

---

apiVersion: v1
data:
  postgres-init.sql: |
    CREATE EXTENSION IF NOT EXISTS pg_trgm;
    CREATE EXTENSION IF NOT EXISTS pg_stat_statements;
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: pg-init-sql-106a59094d5cd047cb07f39e68db563a

---

apiVersion: v1
data:
  db-backup.sh: |
    #!/usr/bin/env bash

    POSTGRES_MAX_BACKUP_NUMBER=3
    POSTGRES_DB=contrax1
    POSTGRES_USER=contrax1

    BACKUP_DIR=/backup/db
    BACKUP_FILE=backup_$(date +%Y-%m-%d_%H-%M-%S).backup
    MAX_BACKUP_NUMBER=$((${POSTGRES_MAX_BACKUP_NUMBER}+1))

    if [ ! -z ${POSTGRES_MAX_BACKUP_NUMBER} ] && [ ${POSTGRES_MAX_BACKUP_NUMBER} -gt 0 ]; then
        mkdir -p ${BACKUP_DIR}
        pushd ${BACKUP_DIR}

        mkdir -p ./tmp

        pg_dump --dbname=${POSTGRES_DB} --username=${POSTGRES_USER} --format=c -Z 9 --file=./tmp/${BACKUP_FILE}
        mv ./tmp/${BACKUP_FILE} ${BACKUP_FILE}
        rm -r ./tmp

        ls -tp | grep -v '/$' | tail -n +${MAX_BACKUP_NUMBER} | xargs -I {} rm -- {}

        popd
    fi
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: db-backup-configmap

---

apiVersion: v1
data:
  pg_hba.conf: |
    # TYPE  DATABASE        USER            ADDRESS                 METHOD

    # "local" is for Unix domain socket connections only
    local   all             all                                     trust
    local   all             postgres                                trust
    # IPv4 local connections:
    host    all             all             127.0.0.1/32            trust
    host	all		all		0.0.0.0/0		md5
    # IPv6 local connections:
    host    all             all             ::1/128                 trust
    # Allow replication connections from localhost, by a user with the
    # replication privilege.
    local   replication     all                                     trust
    host    replication     all             127.0.0.1/32            trust
    host    replication     all             ::1/128                 trust
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: pg-hba-configmap

---
{{ end }}

#contrax-elasticsearch

apiVersion: v1
data:
  elasticsearch.yml: |-
    cluster.name: docker_cluster
    node.name: ${HOSTNAME}
    node.master: true
    node.data: true
    node.ingest: true
    node.max_local_storage_nodes: 1
    path.data: /usr/share/elasticsearch/data
    network.host: ${HOSTNAME}
    network.bind_host: 0.0.0.0
    transport.tcp.port: 9300
    http.port: 9200
    cluster.initial_master_nodes: ["${HOSTNAME}"]
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: elasticsearch-yaml-config

---

#contrax-elastalert

apiVersion: v1
data:
  config.yaml: |-
    rules_folder: /rules
    run_every:
      minutes: 1
    buffer_time:
      minutes: 15
    es_host: contrax-elasticsearch
    es_port: 9200
    smtp_host: localhost
    smtp_port: 587
    smtp_ssl: False
    smtp_auth_file: /elastalert-smtp-auth.yaml
    from_addr: admin@localhost
    writeback_index: elastalert_status
    writeback_alias: elastalert_alerts
    alert_time_limit:
      days: 2
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: elastalert-config-yaml-configmap

---

apiVersion: v1
data:
  elastalert.yaml: |-
    rules_folder: /rules
    run_every:
      minutes: 1
    buffer_time:
      minutes: 15
    es_host: contrax-elasticsearch
    es_port: 9200
    smtp_host: localhost
    smtp_port: 587
    smtp_ssl: False
    smtp_auth_file: /elastalert-smtp-auth.yaml
    from_addr: admin@localhost
    writeback_index: elastalert_status
    writeback_alias: elastalert_alerts
    alert_time_limit:
      days: 2
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: elastalert-server-yaml-configmap

---

apiVersion: v1
data:
  config.json: |-
    {
      "appName": "elastalert-server",
      "port": 3030,
      "wsport": 3333,
      "elastalertPath": "/opt/elastalert",
      "verbose": false,
      "es_debug": false,
      "debug": false,
      "rulesPath": {
        "relative": false,
        "path": "/rules"
      },
      "templatesPath": {
        "relative": false,
        "path": "/rule_templates"
      },
      "es_host": "contrax-elasticsearch",
      "es_port": 9200,
      "smtp_host": "localhost",
      "smtp_port": "587",
      "smtp_ssl": "False",
      "smtp_auth_file": "/elastalert-smtp-auth.yaml",
      "from_addr": "admin@localhost",
      "writeback_index": "elastalert_status"
    }
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: elastalert-server-config-json-configmap

---

apiVersion: v1
data:
  elastalert-smtp-auth.yaml: |-
    user: ""
    password: ""
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: elastalert-smtp-auth-yaml-configmap

---

#contrax-filebeat

apiVersion: v1
data:
  filebeat.yml: |-
    filebeat.modules:
    - module: nginx
      access:
        enabled: true
        var.paths: ["/data/nginx_logs/access.log*"]
      error:
        enabled: true
        var.paths: ["/data/nginx_logs/error.log*"]
    - module: postgresql
      log:
        enabled: true
        var.paths: ["/data/pg_data/pg_log/*.log*"]
    filebeat.prospectors:
    - type: log
      paths:
        - /data/logs/celery-*.log_json
      fields:
        logger: celery
      json.keys_under_root: true
      json.add_error_key: true
      json.overwrite_keys: true
    - type: log
      paths:
        - /data/logs/django-*.log_json
      fields:
        logger: django
      json.keys_under_root: true
      json.add_error_key: true
      json.overwrite_keys: true
    - type: log
      paths:
        - /data/logs/db-*.log_json
      fields:
        logger: db
      json.keys_under_root: true
      json.add_error_key: true
      json.overwrite_keys: true
    - type: log
      paths:
        - /data/logs/frontend-*.log_json
      fields:
        logger: frontend
      json.keys_under_root: true
      json.add_error_key: true
      json.overwrite_keys: true

    # If changing index names here - don't forget to change them in settings.py.
    # Celery task logs are loaded in Django UI by requesting Elasticsearch
    # with index pattern specified in settings.LOGGING_ELASTICSEARCH_INDEX_TEMPLATE

    output.elasticsearch:
      hosts: ["http://contrax-elasticsearch:9200"]
      index: "filebeat-%{[beat.version]}-%{+yyyy.MM.dd}"

    setup.template.name: "filebeat"
    setup.template.pattern: "filebeat-*"
    setup.dashboards.index: "filebeat-*"
    setup.template.overwrite: true

    setup.dashboards.enabled: true

    setup.kibana.host: "contrax-kibana"
    setup.kibana.protocol: "http"
    #setup.kibana.path: "/kibana"
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: filebeat-1b84ea4a4555e9c33f0ea04461533050

---

#contrax-kibana

apiVersion: v1
data:
  kibana.yml: |-
    server.name: kibana
    server.host: "0"
    server.basePath: "/kibana"
    elasticsearch.hosts: [ "http://contrax-elasticsearch:9200" ]
    elastalert-kibana-plugin.serverHost: "contrax-elastalert"
    elastalert-kibana-plugin.serverPort: 3030
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: contrax-kibana-conf-configmap

---

#contrax-metricbeat

apiVersion: v1
data:
  metricbeat.yml: |-
    metricbeat.modules:
    - module: system
      metricsets:
        - cpu
        - filesystem
        - memory
        - network
        - process
      enabled: true
      period: 10s
      processes: ['.*']
      cpu_ticks: false
    - module: docker
      metricsets: ["container", "cpu", "diskio", "healthcheck", "info", "memory", "network"]
      hosts: ["unix:///var/run/docker.sock"]
      period: 10s
    - module: postgresql
      metricsets:
        - database
        - bgwriter
        - activity
      period: 10s
      hosts: ["postgres://contrax-db:5432"]
      username: contrax1
      password: contrax1
    - module: redis
      metricsets: ["info", "keyspace"]
      period: 10s
      hosts: ["contrax-redis:6379"]
    - module: elasticsearch
      metricsets: ["node", "node_stats"]
      period: 10s
      hosts: ["http://contrax-elasticsearch:9200"]
    - module: rabbitmq
      metricsets: ["node", "queue"]
      period: 10s
      hosts: ["contrax-rabbitmq:15672"]
      username: contrax1
      password: contrax1

    output.elasticsearch:
      hosts: ["http://contrax-elasticsearch:9200"]
      index: "metricbeat-%{[beat.version]}-%{+yyyy.MM.dd}"

    setup.template.name: "metricbeat"
    setup.template.pattern: "metricbeat-*"
    setup.dashboards.index: "metricbeat-*"
    setup.template.overwrite: true

    setup.dashboards.enabled: true

    setup.kibana.host: "contrax-kibana"
    setup.kibana.protocol: "http"
    #setup.kibana.path: "/kibana"
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: metricbeat-99b4e27a4f53a7c722f3e17e204d86c2

---

#contrax-nginx

apiVersion: v1
data:
  nginx-customer.conf: |
    # no customer config included
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: nginx-cust-295d861acdf477ca42d4d66a2a29680a

---

#contrax-jupyter

apiVersion: v1
data:
  jupyter_notebook_config.py: |-
    c.NotebookApp.allow_origin = '*'
    c.NotebookApp.base_url = '/jupyter'
    c.NotebookApp.port = 8888
    c.NotebookApp.notebook_dir = '/contraxsuite_services/notebooks'
    c.NotebookApp.disable_check_xsrf = True
    c.NotebookApp.iopub_data_rate_limit = 10000000
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: jupyter-notebook-config-py-configmap

---

#contrax-powa-web

apiVersion: v1
data:
  powa-web.conf: |-
    servers={
      'main': {
        'host': 'contrax-db',
        'port': '5432',
        'database': 'powa'
      }
    }
    cookie_secret="KAJsuasa&&As6a^Sfdsaxzc6sa"
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: powa-web-configmap
